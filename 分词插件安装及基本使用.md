1.elasticsearch 分词插件安装
（1）ansj插件安装
	plugin install http://maven.nlpcn.org/org/ansj/elasticsearch-analysis-ansj/2.3.4/elasticsearch-analysis-ansj-2.3.4-release.zip
	如果你想把ansj作为你的默认分词需要在elasticsearch.yml加入如下配置:
	#默认分词器,索引
	index.analysis.analyzer.default.type: index_ansj

	#默认分词器,查询

	index.analysis.analyzer.default_search.type: query_ansj

	编译安装

	第一步，你要有一个elasticsearch的服务器(废话) 版本2.1.1

	第二步，把代码clone到本地

	第三步，mvn clean install

	第四步，进入$Project_Home/target/releases 目录，

	第五步，拷贝$Project_Home/target/releases/目录下的zip包到解压到$ES_HOME/plugins目录下

	现在,你的es集群已经有下面三个名字的analyzer

	index_ansj (建议索引使用)
	query_ansj (建议搜索使用)
	dic_ansj
	三个名字的tokenizer

	index_ansj (建议索引使用)
	query_ansj (建议搜索使用)
	dic_ansj
	分词文件配置:

	在这里我说一下，在插件里我写了一些默认配置，如果你也可以接受我的默认配置，关于ansj就完全不用配置了，或者只修改你需要的配置。下面的代码目录都是相对es的config目录，有几点需要注意一下:

	ansj的核心词典是和插件一起安装的在插件目录下面
	由于使用redis的pubsub功能，需要相关权限控制，安装的时候必须获得允许，而2.3.3.2以前的版本需要加./elasticsearch -Des.security.manager.enabled=false参数才能解决
	请慎重使用redis的pubsub功能
	注意：这里的ansj目录是在elasticsearch家目录的config目录下的
	## ansj配置
	ansj:
	 dic_path: "ansj/dic/user/" ##用户词典位置
	 ambiguity_path: "ansj/dic/ambiguity.dic" ##歧义词典
	 enable_name_recognition: true ##人名识别
	 enable_num_recognition: true ##数字识别
	 enable_quantifier_recognition: false ##量词识别
	 enabled_stop_filter: true ##是否基于词典过滤
	 enable_skip_user_define: false ## 是否用户词典不加载相同的词
	 stop_path: "ansj/dic/stopLibrary.dic" ##停止过滤词典
	## redis 不是必需的
	redis:
	 pool:
	 maxactive: 20
	 maxidle: 10
	 maxwait: 100
	 testonborrow: true
	 ip: 10.0.85.51:6379
	 timeout: 2000
	 #password: "******" ## 不要添加这个配置，除非redis需要权限认证
	 channel: ansj_term ## publish时的channel名称
	 write:
	   dic: "ext.dic" ## 如果有使用redis的pubsub方式更新词典。如果没有配置，默认使用的是$ES_HOME/config/ansj/dic/user/ext.dic



	查询分词
	可以使用开头我提供的http接口来查看分词效果
	http://127.0.0.1:9200/your_index/_analyze?text=%E5%85%AD%E5%91%B3%E5%9C%B0%E9%BB%84%E4%B8%B8%E8%BD%AF%E8%83%B6%E5%9B%8A&analyzer=index_ansj
	上面的句子最后有个analyzer=index_ansj,表示的是索引的分词
	这里一共有提供三种分词：
		dic_ansj	表示用户自定义词典优先的分词方式
		index_ansj	是索引分词，尽可能分词出所有结果
		query_ansj	是搜索分词，是索引分词的子集

	然后通过redis发布一个新词看看 追加新词

	redis-cli
	publish ansj_term u:c:视康

	是不是分词发生了变化 删除词条

	redis-cli
	publish ansj_term u:d:视康
	又回来了

	然后通过redis发布一个歧义词 追加歧义词

	redis-cli
	publish ansj_term a:c:减肥瘦身-减肥,nr,瘦身,v
	是不是分词发生了变化 删除歧义词

	redis-cli
	publish ansj_term a:d:减肥瘦身


(2)ik分词
	安装：
	 git clone https://github.com/medcl/elasticsearch-analysis-ik
	 cd elasticsearch-analysis-ik
	 git checkout tags/{version}
	 mvn clean
	 mvn compile
	 mvn package
	 拷贝和解压release下的文件: #{project_path}/elasticsearch-analysis-ik/target/releases/elasticsearch-analysis-ik-*.zip 到你的 elasticsearch 插件目录, 如: plugins/ik 重启elasticsearch

	 分词测试失败 请在某个索引下调用analyze接口测试,而不是直接调用analyze接口 如:http://localhost:9200/your_index/_analyze?text=中华人民共和国MN&tokenizer=my_ik
	 ik_max_word: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；

	 ik_smart: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。
	 例如：
	 http://127.0.0.1:9200/your_index/_analyze?text=六味地黄丸胶囊&analyzer=ik_max_word

	 Dictionary Configuration

	 IKAnalyzer.cfg.xml can be located at {conf}/analysis-ik/config/IKAnalyzer.cfg.xml or {plugins}/elasticsearch-analysis-ik-*/config/IKAnalyzer.cfg.xml

	 <?xml version="1.0" encoding="UTF-8"?>
	 <!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
	 <properties>
	     <comment>IK Analyzer 扩展配置</comment>
	     <!--用户可以在这里配置自己的扩展字典 -->
	     <entry key="ext_dict">custom/mydict.dic;custom/single_word_low_freq.dic</entry>
	     <!--用户可以在这里配置自己的扩展停止词字典-->
	     <entry key="ext_stopwords">custom/ext_stopword.dic</entry>
	     <!--用户可以在这里配置远程扩展字典 -->
	     <entry key="remote_ext_dict">location</entry>
	     <!--用户可以在这里配置远程扩展停止词字典-->
	     <entry key="remote_ext_stopwords">http://xxx.com/xxx.dic</entry>
	 </properties>
	热更新 IK 分词使用方法

	目前该插件支持热更新 IK 分词，通过上文在 IK 配置文件中提到的如下配置

		<!--用户可以在这里配置远程扩展字典 -->
		<entry key="remote_ext_dict">location</entry>
		<!--用户可以在这里配置远程扩展停止词字典-->
		<entry key="remote_ext_stopwords">location</entry>
	其中 location 是指一个 url，比如 http://yoursite.com/getCustomDict，该请求只需满足以下两点即可完成分词热更新。

	该 http 请求需要返回两个头部(header)，一个是 Last-Modified，一个是 ETag，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。

	该 http 请求返回的内容格式是一行一个分词，换行符用 \n 即可。

	满足上面两点要求就可以实现热更新分词了，不需要重启 ES 实例。

	可以将需自动更新的热词放在一个 UTF-8 编码的 .txt 文件里，放在 nginx 或其他简易 http server 下，当 .txt 文件修改时，http server 会在客户端请求该文件时自动返回相应的 Last-Modified 和 ETag。可以另外做一个工具来从业务系统提取相关词汇，并更新这个 .txt 文件。



测试

创建测试索引
curl -XPUT 127.0.0.1:9200/test -d '{
    "settings" : {
            "number_of_shards" : 1,
	    "number_of_replicas" : 0

	},
	"mappings" : {
		"test" : {
		 	"_all" : { "enabled" : false },
			"properties" : {
				"name" : { "type" : "string", "analyzer" : "index_ansj", "search_analyzer" : "query_ansj" }
			}
											        }
											}
										}'
										添加索引内容
										curl -XPUT 'http://127.0.0.1:9200/test/test/1' -d '{
											"name" : "中国人民万岁",
											"post_date" : "2009-11-15T14:12:12",
											"message" : "trying out Elasticsearch"
											}'
										查询索引
										浏览器访问:
										http://127.0.0.1:9200/test/test/_search?q=name:中国
